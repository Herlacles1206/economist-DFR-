{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) 2020, salesforce.com, inc.  \n",
    "All rights reserved.  \n",
    "SPDX-License-Identifier: BSD-3-Clause  \n",
    "For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab\n",
    "\n",
    "Try this notebook on [Colab](http://colab.research.google.com/github/salesforce/ai-economist/blob/master/tutorials/optimal_taxation_theory_and_simulation.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial demonstrates how the economic simulation can be used to study the problem of **optimal taxation**.\n",
    "\n",
    "We begin with a brief introduction of optimal taxation and then show how we implement it in simulation.  \n",
    "\n",
    "For further reading, check out the AI Economist [paper](https://arxiv.org/abs/2004.13332) and [blog post](https://blog.einstein.ai/the-ai-economist/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Taxation: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadly speaking, optimal taxation is concerned with how to best use taxes to achieve some socially desirable economic outcome, such as a society that is both prosperous and equitable.\n",
    "\n",
    "Much of the theory around this problem builds on the models developed by James Mirrlees and colleagues in the 1970s (see [Mirrlees, 1971](https://www.jstor.org/stable/2296779?seq=1)). There are several key concepts throughout this line of work which we will introduce below:\n",
    "\n",
    "* **A population of income earners**: workers in the economy, who earn income by performing labor.\n",
    "* **Skill** (or wage): the amount of income a worker is able to receive per unit labor.\n",
    "* **Utility**: the happiness each worker experiences and tries to maximize.\n",
    "* **A social planner**, which sets income tax rates and creates income redistribution.\n",
    "* **Social welfare**: the objective the planner tries to maximize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Workers: Skill, Labor, Income, and Utility\n",
    "\n",
    "- Each worker wants to maximize his/her own happiness (utility).\n",
    "- Each worker gains utility from having money and loses utility from doing work.\n",
    "- We also assume that there is **diminishing marginal utility** from money and/or **increasing marginal cost** from doing work.\n",
    "- This means that there is a _sweet spot_ in the amount of work done/income earned where utility is maximized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore this with some code. We define methods that implement income and utility, using skill as an input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def income_earned(labor, skill):\n",
    "    \"\"\"Income is amount of work (labor) times skill.\"\"\"\n",
    "    return labor * skill\n",
    "\n",
    "def utility(labor, skill):\n",
    "    \"\"\"Utility is convex increasing in income and linearly decreasing in amount of work (labor).\"\"\"\n",
    "\n",
    "    def isoelastic_utility(z, eta=0.35):\n",
    "        \"\"\"Utility gained from income z: https://en.wikipedia.org/wiki/Isoelastic_utility\"\"\"\n",
    "        return (z**(1-eta) - 1) / (1 - eta)\n",
    "    \n",
    "    income = income_earned(labor, skill)\n",
    "    utility_from_income = isoelastic_utility(income)\n",
    "    disutility_from_labor = labor\n",
    "    \n",
    "    # Total utility is utility from income minus disutility incurred from working\n",
    "    return utility_from_income - disutility_from_labor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_utility_curve(skill, ax):\n",
    "    \"\"\"Plot the curve relating labor performed to total utility.\"\"\"\n",
    "    labor_array = np.linspace(0, 1000, 501)\n",
    "    utility_array = utility(labor_array, skill)\n",
    "    ax.plot(labor_array, utility_array, label=\"Skill = {}\".format(skill))\n",
    "    ax.plot(labor_array[np.argmax(utility_array)], np.max(utility_array), 'k*', markersize=10)\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(16, 6))\n",
    "\n",
    "for skill in [10, 15, 20, 30]:\n",
    "    plot_utility_curve(skill, ax)\n",
    "    \n",
    "ax.set_xlabel(\"Labor Performed\", fontsize=20)\n",
    "ax.set_ylabel(\"Utility Experienced\", fontsize=20)\n",
    "ax.set_xlim(left=0, right=1000)\n",
    "ax.set_ylim(bottom=0)\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in this visual demonstration, we've brought in the concept of **skill**, which describes how much income a worker makes per unit of labor. Skill is very important for a couple reasons:\n",
    "* First, the amount of income a worker gets affects how much he/she will choose to work (and, choose to earn) to maximize utility.  \n",
    "More skill -> more labor -> more total income --> more utility.\n",
    "* Second, **skill is not equal among workers**. Moreover, the market may value the labor of one worker much more than that of another, leading to **income inequality** and large differences in how much utility each agent is able to experience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social Planner: taxes and social welfare\n",
    "How can we create a more equitable economy?\n",
    "- A naive approach may be to move money from rich workers to poor workers through **taxation and redistribution**.\n",
    "- The challenge there is that workers choose how much to work based on how much they earn from working, and taxes (from the worker's perspective) cause that to go down.  \n",
    "- **In other words, higher income tax -> less overall income to tax.**\n",
    "\n",
    "Just like workers want to maximize their own utility, the social planner has its own objective called **social welfare**. It is up to society to settle on a good definition of social welfare. \n",
    "\n",
    "The challenge of optimal taxation arises when social welfare emphasizes both equality (similar incomes/utilities) and productivity (total income/utility), *because there is an inherent trade-off between the two*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory to Simulation\n",
    "\n",
    "Economic theory has studied this problem in various modeling settings, but it remains hard to mathematically derive the optimal tax policy. \n",
    "\n",
    "- Simple economies yield [mathematically tractable tax formulas](https://eml.berkeley.edu/~saez/saez-stantchevaJpubE18optKtax.pdf), but fail to capture the rich dynamics of a more realistic economy.\n",
    "- Complex economies can describe economic processes and actors more accurately (e.g., [the New Dynamic Public Finance](https://econpapers.repec.org/bookchap/puppbooks/9222.htm)), but lead to tax formulas that are hard to explicitly calculate.\n",
    "\n",
    "Simulation offers a lens to understand these dynamics and enables us to use reinforcement learning to *learn* optimal tax policies. However, a good simulation is grounded in the same intuitions underlying simpler, theoretical models.\n",
    "\n",
    "Below, we unpack our approach for studying the optimal taxation problem in simulation, and highlight how we can validate emergent economic behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulating the optimal taxation problem\n",
    "\n",
    "Let's start with the full environment and then zoom in as needed to see how it is grounded in the key concepts of optimal taxation.  \n",
    "\n",
    "Before moving forward, you should be familiar with the basics for how to build simulation environments in our framework. Check out:\n",
    "\n",
    "- [Basics tutorial](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_basic.ipynb): this tutorial shows how to interact with the simulation.\n",
    "- [Advanced tutorial](https://github.com/salesforce/ai-economist/blob/master/tutorials/economic_simulation_advanced.ipynb): this tutorial shows how the simulation is composed out of flexible building blocks, such as Resources, Agents, and Components.\n",
    "- The code and documentation for the [core classes](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/base)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies:\n",
    "You can install the ai-economist package using \n",
    "- the pip package manager OR\n",
    "- by cloning the ai-economist package and installing the requirements (we shall use this when running on Colab):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, signal, sys, time\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/salesforce/ai-economist.git\n",
    "\n",
    "    %cd ai-economist\n",
    "    !pip install -e .\n",
    "    \n",
    "    # Restart the Python runtime to automatically use the installed packages\n",
    "    print(\"\\n\\nRestarting the Python runtime! Please (re-)run the cells below.\")\n",
    "    time.sleep(1)\n",
    "    os.kill(os.getpid(), signal.SIGKILL)\n",
    "else:\n",
    "    !pip install ai-economist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import foundation from source\n",
    "from ai_economist import foundation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the configuration of the environment that will be built\n",
    "\n",
    "env_config = {\n",
    "    # ===== SCENARIO CLASS =====\n",
    "    # Which Scenario class to use: the class's name in the Scenario Registry (foundation.scenarios).\n",
    "    # The environment object will be an instance of the Scenario class.\n",
    "    'scenario_name': 'layout_from_file/simple_wood_and_stone',\n",
    "    \n",
    "    # ===== COMPONENTS =====\n",
    "    # Which components to use (specified as list of (\"component_name\", {component_kwargs}) tuples).\n",
    "    #     \"component_name\" refers to the Component class's name in the Component Registry (foundation.components)\n",
    "    #     {component_kwargs} is a dictionary of kwargs passed to the Component class\n",
    "    # The order in which components reset, step, and generate obs follows their listed order below.\n",
    "    'components': [\n",
    "        # (1) Building houses\n",
    "        ('Build', dict(skill_dist=\"pareto\", payment_max_skill_multiplier=3)),\n",
    "        # (2) Trading collectible resources\n",
    "        ('ContinuousDoubleAuction', dict(max_num_orders=5)),\n",
    "        # (3) Movement and resource collection\n",
    "        ('Gather', dict()),\n",
    "        # (4) Income tax & lump-sum redistribution\n",
    "        ('PeriodicBracketTax', dict(bracket_spacing=\"us-federal\", period=100))\n",
    "    ],\n",
    "    \n",
    "    # ===== SCENARIO CLASS ARGUMENTS =====\n",
    "    # (optional) kwargs that are added by the Scenario class (i.e. not defined in BaseEnvironment)\n",
    "    'env_layout_file': \"quadrant_25x25_20each_30clump.txt\",\n",
    "    'fixed_four_skill_and_loc': True,\n",
    "    \n",
    "    # ===== STANDARD ARGUMENTS ======\n",
    "    # kwargs that are used by every Scenario class (i.e. defined in BaseEnvironment)\n",
    "    'n_agents': 4,          # Number of non-planner agents (must be >1)\n",
    "    'world_size': [25, 25], # [Height, Width] of the env world\n",
    "    'episode_length': 1000, # Number of timesteps per episode\n",
    "    \n",
    "    # In multi-action-mode, the policy selects an action for each action subspace (defined in component code).\n",
    "    # Otherwise, the policy selects only 1 action.\n",
    "    'multi_action_mode_agents': False,\n",
    "    'multi_action_mode_planner': True,\n",
    "    \n",
    "    # When flattening observations, concatenate scalar & vector observations before output.\n",
    "    # Otherwise, return observations with minimal processing.\n",
    "    'flatten_observations': False,\n",
    "    # When Flattening masks, concatenate each action subspace mask into a single array.\n",
    "    # Note: flatten_masks = True is required for masking action logits in the code below.\n",
    "    'flatten_masks': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following line of code creates an instance of a **Scenario** object. More specifically, ```make_env_instance``` finds the Scenario class associated with name ```'layout_from_file/simple_wood_and_stone'``` and returns an instance of it using the rest of ```env_config``` as keyword arguments during construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = foundation.make_env_instance(**env_config)\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual workers (\"agents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```env_config``` specifies that our environment should include 4 individual worker agents (```env_config['n_agents']```).\n",
    "\n",
    "Each worker agent is represented as a separate Python object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.world.agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each agent object stores its own state information, like location, endowments, etc.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.world.agents[0].state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worker agents are keyed by numerical indices. These can be used, for example, to look them up and to associate particular agent objects with specific observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent0 = env.get_agent(agent_idx=0)\n",
    "agent0.idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(obs.keys())\n",
    "print(obs['0'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility\n",
    "\n",
    "Basic economic intuition (see above) is that each agent attempts to maximize its own utility. In a reinforcement learning context, this means that each agent wants to maximize the sum of marginal utilities experienced at each future timestep. **In other words, we want to use marginal utility as the agent's reward.**\n",
    "\n",
    "Each Scenario class must implement the reward function in method ```compute_rewards```. The Scenario class that ```env``` belongs to makes use of the following function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def isoelastic_coin_minus_labor(\n",
    "    coin_endowment, total_labor, isoelastic_eta, labor_coefficient\n",
    "):\n",
    "    \"\"\"Agent utility, concave increasing in coin and linearly decreasing in labor.\n",
    "\n",
    "    Args:\n",
    "        coin_endowment (float, ndarray): The amount of coin owned by the agent(s).\n",
    "        total_labor (float, ndarray): The amount of labor performed by the agent(s).\n",
    "        isoelastic_eta (float): Constant describing the shape of the utility profile\n",
    "            with respect to coin endowment. Must be between 0 and 1. 0 yields utility\n",
    "            that increases linearly with coin. 1 yields utility that increases with\n",
    "            log(coin). Utility from coin uses:\n",
    "                https://en.wikipedia.org/wiki/Isoelastic_utility\n",
    "        labor_coefficient (float): Constant describing the disutility experienced per\n",
    "            unit of labor performed. Disutility from labor equals:\n",
    "                labor_coefficient * total_labor\n",
    "\n",
    "    Returns:\n",
    "        Agent utility (float) or utilities (ndarray).\n",
    "    \"\"\"\n",
    "    # https://en.wikipedia.org/wiki/Isoelastic_utility\n",
    "    assert np.all(coin_endowment >= 0)\n",
    "    assert 0 <= isoelastic_eta <= 1.0\n",
    "\n",
    "    # Utility from coin endowment\n",
    "    if isoelastic_eta == 1.0:  # dangerous\n",
    "        util_c = np.log(np.max(1, coin_endowment))\n",
    "    else:  # isoelastic_eta >= 0\n",
    "        util_c = (coin_endowment ** (1 - isoelastic_eta) - 1) / (1 - isoelastic_eta)\n",
    "\n",
    "    # disutility from labor\n",
    "    util_l = total_labor * labor_coefficient\n",
    "\n",
    "    # Net utility\n",
    "    util = util_c - util_l\n",
    "\n",
    "    return util\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this is essentially the same definition of utility we used above!\n",
    "\n",
    "```env.compute_rewards``` uses this utility function to compute utilities from agent's coin endowments (```agent.state['inventory']['Coin'] + agent.state['escrow']['Coin']```) and accumulated labor (```agent.state['endogenous']['Labor']```) at the end of each step. It returns as reward the change in utility from the previous step.\n",
    "\n",
    "Let's look at the code that implements this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Excerpts from the Scenario class code:\n",
    "    \n",
    "    def get_current_optimization_metrics(self):\n",
    "        \"\"\"\n",
    "        Compute optimization metrics based on the current state. Used to compute reward.\n",
    "\n",
    "        Returns:\n",
    "            curr_optimization_metric (dict): A dictionary of {agent.idx: metric}\n",
    "                with an entry for each agent (including the planner) in the env.\n",
    "        \"\"\"\n",
    "        curr_optimization_metric = {}\n",
    "        # (for agents)\n",
    "        for agent in self.world.agents:\n",
    "            curr_optimization_metric[agent.idx] = rewards.isoelastic_coin_minus_labor(\n",
    "                coin_endowment=agent.total_endowment(\"Coin\"),\n",
    "                total_labor=agent.state[\"endogenous\"][\"Labor\"],\n",
    "                isoelastic_eta=self.isoelastic_eta,\n",
    "                labor_coefficient=self.energy_weight * self.energy_cost,\n",
    "            )\n",
    "            \n",
    "        # ... omitting for brevity the planner's optimization metric ...\n",
    "        \n",
    "        return curr_optimization_metric\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        \"\"\"\n",
    "        Apply the reward function(s) associated with this scenario to get the rewards\n",
    "        from this step.\n",
    "\n",
    "        Returns:\n",
    "            rew (dict): A dictionary of {agent.idx: agent_obs_dict}. In words,\n",
    "                return a dictionary with an entry for each agent in the environment\n",
    "                (including the planner). For each entry, the key specifies the index of\n",
    "                the agent and the value contains the scalar reward earned this timestep.\n",
    "\n",
    "        Rewards are computed as the marginal utility (agents) or marginal social\n",
    "        welfare (planner) experienced on this timestep. Ignoring discounting,\n",
    "        this means that agents' (planner's) objective is to maximize the utility\n",
    "        (social welfare) associated with the terminal state of the episode.\n",
    "        \"\"\"\n",
    "\n",
    "        # \"curr_optimization_metric\" hasn't been updated yet, so it gives us the\n",
    "        # utility from the last step.\n",
    "        utility_at_end_of_last_time_step = deepcopy(self.curr_optimization_metric)\n",
    "\n",
    "        # compute current objectives and store the values\n",
    "        self.curr_optimization_metric = self.get_current_optimization_metrics()\n",
    "\n",
    "        # reward = curr - prev objectives\n",
    "        rew = {\n",
    "            k: float(v - utility_at_end_of_last_time_step[k])\n",
    "            for k, v in self.curr_optimization_metric.items()\n",
    "        }\n",
    "\n",
    "        return rew\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labor\n",
    "\n",
    "In order to recreate the optimal tax problem, it is not enough simply to use a utility-based reward function. We have to design our simulation such that agents experience a labor cost associated with the actions that ultimately create income. That is, work needs to feel like work!\n",
    "\n",
    "An example is found in the part of the environment that enables workers to move around and gather resources: the ```Gather``` Component class. Below, we highlight the portions of the class's code that associate a labor cost with moving and resource collection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "\n",
    "from ai_economist.foundation.base.base_component import BaseComponent, component_registry\n",
    "\n",
    "\n",
    "@component_registry.add\n",
    "class Gather(BaseComponent):\n",
    "    \"\"\"\n",
    "    Allows mobile agents to move around the world and collect resources and prevents\n",
    "    agents from moving to invalid locations.\n",
    "\n",
    "    Can be configured to include collection skill, where agents have heterogeneous\n",
    "    probabilities of collecting bonus resources without additional labor cost.\n",
    "\n",
    "    Args:\n",
    "        move_labor (float): Labor cost associated with movement. Must be >= 0.\n",
    "            Default is 1.0.\n",
    "        collect_labor (float): Labor cost associated with collecting resources. This\n",
    "            cost is added (in addition to any movement cost) when the agent lands on\n",
    "            a tile that is populated with resources (triggering collection).\n",
    "            Must be >= 0. Default is 1.0.\n",
    "        skill_dist (str): Distribution type for sampling skills. Default (\"none\")\n",
    "            gives all agents identical skill equal to a bonus prob of 0. \"pareto\" and\n",
    "            \"lognormal\" sample skills from the associated distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"Gather\"\n",
    "    required_entities = [\"Coin\", \"House\", \"Labor\"]\n",
    "    agent_subclasses = [\"BasicMobileAgent\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *base_component_args,\n",
    "        move_labor=1.0,\n",
    "        collect_labor=1.0,\n",
    "        skill_dist=\"none\",\n",
    "        **base_component_kwargs\n",
    "    ):\n",
    "        super().__init__(*base_component_args, **base_component_kwargs)\n",
    "\n",
    "        self.move_labor = float(move_labor)\n",
    "        assert self.move_labor >= 0\n",
    "\n",
    "        self.collect_labor = float(collect_labor)\n",
    "        assert self.collect_labor >= 0\n",
    "\n",
    "\n",
    "    def component_step(self):\n",
    "        \"\"\"\n",
    "        See base_component.py for detailed description.\n",
    "\n",
    "        Move to adjacent, unoccupied locations. Collect resources when moving to\n",
    "        populated resource tiles, adding the resource to the agent's inventory and\n",
    "        de-populating it from the tile.\n",
    "        \"\"\"\n",
    "        world = self.world\n",
    "\n",
    "        for agent in world.get_random_order_agents():\n",
    "\n",
    "            if self.name not in agent.action:\n",
    "                return\n",
    "            action = agent.get_component_action(self.name)\n",
    "\n",
    "            r, c = [int(x) for x in agent.loc]\n",
    "\n",
    "            if action == 0:  # NO-OP!\n",
    "                new_r, new_c = r, c\n",
    "\n",
    "            elif action <= 4:\n",
    "                if action == 1:  # Left\n",
    "                    new_r, new_c = r, c - 1\n",
    "                elif action == 2:  # Right\n",
    "                    new_r, new_c = r, c + 1\n",
    "                elif action == 3:  # Up\n",
    "                    new_r, new_c = r - 1, c\n",
    "                else:  # action == 4, # Down\n",
    "                    new_r, new_c = r + 1, c\n",
    "\n",
    "                # Attempt to move the agent (if the new coordinates aren't accessible,\n",
    "                # nothing will happen)\n",
    "                new_r, new_c = world.set_agent_loc(agent, new_r, new_c)\n",
    "\n",
    "                # If the agent did move, incur the labor cost of moving\n",
    "                if (new_r != r) or (new_c != c):\n",
    "                    agent.state[\"endogenous\"][\"Labor\"] += self.move_labor # <----- LABOR HERE...\n",
    "\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            for resource, health in world.location_resources(new_r, new_c).items():\n",
    "                if health >= 1:\n",
    "                    n_gathered = 1 + (rand() < agent.state[\"bonus_gather_prob\"])\n",
    "                    agent.state[\"inventory\"][resource] += n_gathered\n",
    "                    world.consume_resource(resource, new_r, new_c)\n",
    "                    # Incur the labor cost of collecting a resource\n",
    "                    agent.state[\"endogenous\"][\"Labor\"] += self.collect_labor # <----- ...AND HERE\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In ```component_step```, each time an agent changes its location it adds ```self.move_labor``` to its accumulated labor, and each time it collects resources from the world it adds ```self.collect_labor``` to its accumulated labor.\n",
    "\n",
    "Resources are valuable because agents can use them to generate income, either by using them to build houses or by selling them to other agents. \n",
    "\n",
    "As such, and in keeping with the intuitions of the simpler theoretical model, **agents earn income by performing labor.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill\n",
    "\n",
    "Recall that ***skill* describes the amount of income an agent is able to receive per unit of labor it performs.** In our environment, there is no direct conversion from labor to income: doing things that cost labor don't necessarily yield income; however, earning income requires taking some actions (which cost labor).  \n",
    "\n",
    "So, in our simulation, an agent's *skill* is better understood as the ratio of income-to-labor that it is able to achieve. We don't control this ratio directly but we can still create a population of heterogeneously skilled agents by allowing some agents to earn more than others for particular behaviors.  \n",
    "\n",
    "The best example is found in the part of the environment that enables worker agents to build houses: the ```Build``` Component class. The code snippets below demonstrate how this is achieved.\n",
    "\n",
    "Starting with the ```Build.__init__``` method, each of the 4 custom arguments influence agents' *skill* as defined above. The method code itself is just setting up some parameters that will come into play. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "from ai_economist.foundation.base.base_component import BaseComponent, component_registry\n",
    "\n",
    "\n",
    "@component_registry.add\n",
    "class Build(BaseComponent):\n",
    "    \"\"\"\n",
    "    Allows mobile agents to build house landmarks in the world using stone and wood,\n",
    "    earning income.\n",
    "\n",
    "    Can be configured to include heterogeneous building skill where agents earn\n",
    "    different levels of income when building.\n",
    "\n",
    "    Args:\n",
    "        payment (int): Default amount of coin agents earn from building.\n",
    "            Must be >= 0. Default is 10.\n",
    "        payment_max_skill_multiplier (int): Maximum skill multiplier that an agent\n",
    "            can sample. Must be >= 1. Default is 1.\n",
    "        skill_dist (str): Distribution type for sampling skills. Default (\"none\")\n",
    "            gives all agents identical skill equal to a multiplier of 1. \"pareto\" and\n",
    "            \"lognormal\" sample skills from the associated distributions.\n",
    "        build_labor (float): Labor cost associated with building a house.\n",
    "            Must be >= 0. Default is 10.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"Build\"\n",
    "    mech_type = \"Build\"\n",
    "    required_entities = [\"Wood\", \"Stone\", \"Coin\", \"House\", \"Labor\"]\n",
    "    agent_subclasses = [\"BasicMobileAgent\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        *base_component_args,\n",
    "        payment=10,\n",
    "        payment_max_skill_multiplier=1,\n",
    "        skill_dist=\"none\",\n",
    "        build_labor=10.0,\n",
    "        **base_component_kwargs\n",
    "    ):\n",
    "        super().__init__(*base_component_args, **base_component_kwargs)\n",
    "\n",
    "        self.payment = int(payment)\n",
    "        assert self.payment >= 0\n",
    "\n",
    "        self.payment_max_skill_multiplier = int(payment_max_skill_multiplier)\n",
    "        assert self.payment_max_skill_multiplier >= 1\n",
    "\n",
    "        self.resource_cost = {\"Wood\": 1, \"Stone\": 1}\n",
    "\n",
    "        self.build_labor = float(build_labor)\n",
    "        assert self.build_labor >= 0\n",
    "\n",
    "        self.skill_dist = skill_dist.lower()\n",
    "        assert self.skill_dist in [\"none\", \"pareto\", \"lognormal\"]\n",
    "\n",
    "        self.sampled_skills = {}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skill level for the ```Build``` component is parameterized in a very simple way: **the amount of coin an agent receives when it builds a house.**\n",
    "\n",
    "```Build.get_additional_state_fields``` adds a couple fields to each agent's state dictionary (with placeholder values) for income-per-house.  \n",
    "\n",
    "```Build.additional_reset_steps``` finalizes the build component's reset procedure by actually sampling (if appropriate) build skill parameters for each agent and updating the agent's associated state. The code that generates observations through this component (not shown here) allows agents to observe their own building skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def get_additional_state_fields(self, agent_cls_name):\n",
    "        \"\"\"\n",
    "        See base_component.py for detailed description.\n",
    "\n",
    "        For mobile agents, add state fields for building skill.\n",
    "        \"\"\"\n",
    "        if agent_cls_name not in self.agent_subclasses:\n",
    "            return {}\n",
    "        if agent_cls_name == \"BasicMobileAgent\":\n",
    "            return {\"build_payment\": float(self.payment), \"build_skill\": 1}\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def additional_reset_steps(self):\n",
    "        \"\"\"\n",
    "        See base_component.py for detailed description.\n",
    "\n",
    "        Re-sample agents' building skills.\n",
    "        \"\"\"\n",
    "        world = self.world\n",
    "\n",
    "        self.sampled_skills = {agent.idx: 1 for agent in world.agents}\n",
    "\n",
    "        PMSM = self.payment_max_skill_multiplier\n",
    "\n",
    "        for agent in world.agents:\n",
    "            if self.skill_dist == \"none\":\n",
    "                sampled_skill = 1\n",
    "                pay_rate = 1\n",
    "            elif self.skill_dist == \"pareto\":\n",
    "                sampled_skill = np.random.pareto(4)\n",
    "                pay_rate = np.minimum(PMSM, (PMSM - 1) * sampled_skill + 1)\n",
    "            elif self.skill_dist == \"lognormal\":\n",
    "                sampled_skill = np.random.lognormal(-1, 0.5)\n",
    "                pay_rate = np.minimum(PMSM, (PMSM - 1) * sampled_skill + 1)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            agent.state[\"build_payment\"] = float(pay_rate * self.payment)\n",
    "            agent.state[\"build_skill\"] = float(sampled_skill)\n",
    "\n",
    "            self.sampled_skills[agent.idx] = sampled_skill\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can see below how the Component uses the building skill in its ```component_step``` method, which is responsible for executing the build actions in the environment. When an agent selects the build action (and it is able to actually build), this Component:\n",
    "\n",
    "+ removes the material cost from the agent's inventory (1 stone, 1 wood),\n",
    "+ places a \"House\" landmark where the agent is standing,\n",
    "+ **adds skill-dependent income**, and\n",
    "+ **counts the labor cost associated with building**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    def component_step(self):\n",
    "        \"\"\"\n",
    "        See base_component.py for detailed description.\n",
    "\n",
    "        Convert stone+wood to house+coin for agents that choose to build and can.\n",
    "        \"\"\"\n",
    "        world = self.world\n",
    "\n",
    "        # Apply any building actions taken by the mobile agents\n",
    "        for agent in world.get_random_order_agents():\n",
    "\n",
    "            action = agent.get_component_action(self.name)\n",
    "\n",
    "            # This component doesn't apply to this agent!\n",
    "            if action is None:\n",
    "                continue\n",
    "\n",
    "            # NO-OP!\n",
    "            if action == 0:\n",
    "                pass\n",
    "\n",
    "            # Build! (If you can.)\n",
    "            elif action == 1:\n",
    "                if self.agent_can_build(agent):\n",
    "                    # Remove the resources\n",
    "                    for resource, cost in self.resource_cost.items():\n",
    "                        agent.state[\"inventory\"][resource] -= cost\n",
    "\n",
    "                    # Place a house where the agent is standing\n",
    "                    loc_r, loc_c = agent.loc\n",
    "                    world.create_landmark(\"House\", loc_r, loc_c, agent.idx)\n",
    "\n",
    "                    # Receive payment for the house\n",
    "                    agent.state[\"inventory\"][\"Coin\"] += agent.state[\"build_payment\"]\n",
    "\n",
    "                    # Incur the labor cost for building\n",
    "                    agent.state[\"endogenous\"][\"Labor\"] += self.build_labor\n",
    "\n",
    "            else:\n",
    "                raise ValueError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting ```env_config['fixed_four_skill_and_loc'] = True```, we are telling this Scenario class to ensure that the same 4 building skill levels are randomly permuted between the 4 agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in env.world.agents:\n",
    "    print('Agent {} coin-per-house: {}'.format(agent.idx, agent.state['build_payment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Economic behavior\n",
    "\n",
    "Heterogeneous payoffs from building (see above) yields several key aspects of our economic simulation, including income inequality, different choices of how much to work, and specialization of labor!\n",
    "\n",
    "This is best illustrated by examining the behavior patterns of agents trained to maximize the utility-based reward we detailed above:\n",
    "\n",
    "![Breakdown of trained agents without taxes](assets/breakdown.svg)\n",
    "\n",
    "This visual shows how the game plays out over the course of a single 1000-timestep episode:\n",
    "- **TOP**: The state of the world, including where agents, resources, and houses are.\n",
    "- **MIDDLE**: Each agent's accumulated labor, coin endowment, and resulting utility.\n",
    "- **BOTTOM**: Each agent's total income/expenditures from trading (shown in 25-timestep bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specialization\n",
    "The only difference between agents are their starting locations and building skill. Yet, they exhibit distinct behaviors consistent with **specialization**:\n",
    "- The orange agent, with the highest building skill, buys lots of resources in order to efficiently build.\n",
    "- The dark- and light-blue agents' building skills are too low to justify the high labor cost of building; instead, they gather and sell nearby resources.\n",
    "- The yellow agent is willing to build early on but eventually switches to gathering/selling.\n",
    "\n",
    "### Labor choices\n",
    "Consistent with theory, skill determines the optimal amount of labor (and income). The highest-skilled agent chooses to perform much more labor. Note that in the middle-left plot, Labor is the value in ```agent.state['endogenous']['Labor']``` at each point in time. This combines Labor accumulated through all sources: moving, collecting, trading, and building. The actual *source* of Labor differs between agents based on how they specialize.\n",
    "\n",
    "### Inequality\n",
    "Unequal skill translates into income inequality and an economy where the happiness an agent can experience is limited by its economic opportunity -- that is, the effective skill it is able to achieve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Social planner\n",
    "\n",
    "So far we have shown how this simulation can support the kinds of economic forces/behaviors that lead to undesirable social outcomes -- namely, inequality. We can also use the simulation to look at creating better social outcomes through public policy.\n",
    "\n",
    "To that end, our simulation environment supplies us with an additional agent representing the social planner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.world.planner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From an RL perspective, the planner agent works just like the worker agents: it has a own state, receives observations, takes actions, and earns a reward. But internally worker agents and the planner agent are semantically different and Scenario and Component classes reflect this.\n",
    "\n",
    "As an example, notice that the components yield completely different action sets for worker agents and the planner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent0 = env.get_agent(agent_idx=0) # Worker agent with index 0\n",
    "agent0.action_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "planner = env.get_agent(agent_idx='p') # The planner agent\n",
    "planner.action_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, while each worker agent tries to maximize its own utility, the planner tries to maximize **social welfare**, which captures the social preferences of the society. In this case, we use **coin equality times productivity**.\n",
    "\n",
    "In an RL context, we simply use marginal social welfare as the planner reward, in much the same way we use marginal utility as the worker agent reward. Computing the planner's \"optimization metric\" (see above) uses:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from ai_economist.foundation.scenarios.utils import social_metrics\n",
    "\n",
    "def coin_eq_times_productivity(coin_endowments, equality_weight):\n",
    "    \"\"\"Social welfare, measured as productivity scaled by the degree of coin equality.\n",
    "\n",
    "    Args:\n",
    "        coin_endowments (ndarray): The array of coin endowments for each of the\n",
    "            agents in the simulated economy.\n",
    "        equality_weight (float): Constant that determines how productivity is scaled\n",
    "            by coin equality. Must be between 0 (SW = prod) and 1 (SW = prod * eq).\n",
    "\n",
    "    Returns:\n",
    "        Product of coin equality and productivity (float).\n",
    "    \"\"\"\n",
    "    n_agents = len(coin_endowments)\n",
    "    prod = social_metrics.get_productivity(coin_endowments) / n_agents\n",
    "    equality = equality_weight * social_metrics.get_equality(coin_endowments) + (\n",
    "        1 - equality_weight\n",
    "    )\n",
    "    return equality * prod\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxation\n",
    "\n",
    "And, after all this time, we've arrived at taxation! We're interested in using income taxes & redistribution as a way to improve social welfare.\n",
    "\n",
    "Examining this in our simulation is simply a matter of implementing a Component that adds income taxes & redistribution, which is exactly what ```PeriodicBracketTax``` does.\n",
    "\n",
    "This Component class specifically handles bracketed income taxes with a periodic time structure (like those used in the US economy): it divides the episode into tax periods (like years); at the start of each period tax rates are chosen and announced, and at the end of each period new income is taxed at those rates and evenly redistributed.\n",
    "\n",
    "The planner actions we saw just a bit ago represent the actions afforded by this Component, which lets the planner select one of 21 marginal tax rates to apply to each of the 7 income brackets.\n",
    "\n",
    "This excerpt from the ```PeriodicBracketTax``` code sheds some light on how this is implemented:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "@component_registry.add\n",
    "class PeriodicBracketTax(BaseComponent):\n",
    "    \"\"\"Periodically collect income taxes from agents and do lump-sum redistribution.\n",
    "    \"\"\"\n",
    "\n",
    "    name = \"PeriodicBracketTax\"\n",
    "    component_type = \"PeriodicTax\"\n",
    "    required_entities = [\"Coin\"]\n",
    "    agent_subclasses = [\"BasicMobileAgent\", \"BasicPlanner\"]\n",
    "\n",
    "\n",
    "    def set_new_period_rates_model(self):\n",
    "        \"\"\"Update taxes using actions from the tax model.\"\"\"\n",
    "        if self.disable_taxes:\n",
    "            return\n",
    "\n",
    "        # AI version\n",
    "        for i, bracket in enumerate(self.bracket_cutoffs):\n",
    "            planner_action = self.world.planner.get_component_action(\n",
    "                self.name, \"TaxIndexBracket_{:03d}\".format(int(bracket))\n",
    "            )\n",
    "            if planner_action == 0:\n",
    "                pass\n",
    "            elif planner_action <= self.n_disc_rates:\n",
    "                self.curr_rate_indices[i] = int(planner_action - 1)\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "    def get_n_actions(self, agent_cls_name):\n",
    "        \"\"\"\n",
    "        See base_component.py for detailed description.\n",
    "\n",
    "        If using the \"model_wrapper\" tax model and taxes are enabled, the planner's\n",
    "        action space includes an action subspace for each of the tax brackets. Each\n",
    "        such action space has as many actions as there are discretized tax rates.\n",
    "        \"\"\"\n",
    "        # Only the planner takes actions through this component\n",
    "        if agent_cls_name == \"BasicPlanner\":\n",
    "            if self.tax_model == \"model_wrapper\" and not self.disable_taxes:\n",
    "                # For every bracket, the planner can select one of the discretized\n",
    "                # tax rates.\n",
    "                return [\n",
    "                    (\"TaxIndexBracket_{:03d}\".format(int(r)), self.n_disc_rates)\n",
    "                    for r in self.bracket_cutoffs\n",
    "                ]\n",
    "\n",
    "        # Return 0 (no added actions) if the other conditions aren't met\n",
    "        return 0\n",
    "\n",
    "    def component_step(self):\n",
    "        \"\"\"\n",
    "        See base_component.py for detailed description.\n",
    "\n",
    "        On the first day of each tax period, update taxes. On the last day, enact them.\n",
    "        \"\"\"\n",
    "\n",
    "        # 1. On the first day of a new tax period: Set up the taxes for this period.\n",
    "        if self.tax_cycle_pos == 1:\n",
    "            if self.tax_model == \"model_wrapper\":\n",
    "                self.set_new_period_rates_model()\n",
    "\n",
    "        # 2. On the last day of the tax period: Enact taxes (collect + redistribute)\n",
    "        if self.tax_cycle_pos >= self.period:\n",
    "            self.enact_taxes()\n",
    "            self.tax_cycle_pos = 0\n",
    "\n",
    "        else:\n",
    "            self.taxes.append([])\n",
    "\n",
    "        # increment timestep\n",
    "        self.tax_cycle_pos += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: This excerpt is intended only to show the general logic and how the planner's actions are used to update bracket rates. The [full code](https://github.com/salesforce/ai-economist/blob/master/ai_economist/foundation/components/redistribution.py) is substantially longer, but may be useful to explore if you're interested.\n",
    "\n",
    "There are a few additional details worth pointing out:\n",
    "- Worker agents get to see the taxes that will affect them through the observations generated by the component (```generate_observations```).\n",
    "- The planner is forced to take the NO-OP action on all timesteps except those at the start of each tax period (```generate_masks```)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal taxation\n",
    "\n",
    "Previously, we've seen how we ground the simulation in basic economic theory and intuition, yielding agents with behavior patterns consistent with those theories and intuitions.\n",
    "\n",
    "One such feature of their behavior is that they choose how much to work and earn based on their \"skill\" -- that is, how much income they are able to earn per unit of labor performed.\n",
    "\n",
    "Based on this definition, taxing income reduces skill (from the perspective of the worker agent), so we should expect agents to work (and earn) less when taxes are applied. In fact, this is the central dilemma underlying optimal taxation: **improving equality (through taxation & redistribution) comes at the cost of productivity.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, that's exactly what we see when comparing economies with no income tax (\"Free Market\") to those that use periodic bracketed income taxes. The plot below shows these social metrics when using AI agents and social planner that have been trained to optimize their respective objectives (see [our paper](https://arxiv.org/abs/2004.13332) for the details):\n",
    "\n",
    "![prod_eq_welfare](assets/prod_eq_welfare.svg)\n",
    "\n",
    "This visual shows the total productivity, the income equality, and social welfare (equality * productivity) when we add periodic income taxes and let the worker agents adjust their behavior in response:\n",
    "- **US Federal**: Bracket rates set to follow the US Federal tax rates from 2018.\n",
    "- **Saez Formula**: Rates chosen using an adaptation of the theoretically optimal formula developed by Emmanuel Saez (see [here](https://eml.berkeley.edu/~saez/saez-stantchevaJpubE18optKtax.pdf)).\n",
    "- **AI Economist**: Rates controlled by the **planner** policy trained (alongside the workers) to maximize social welfare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, while taxes + redistribution improve equality, they incur some productivity cost because of the way the worker agents respond to the taxes. But, clearly, some tax schemes negotiate that trade-off better than others!\n",
    "\n",
    "Our economic simulation not only allows us to use RL to demonstrate this concept but also allows us to use RL to learn optimal taxation policies.\n",
    "\n",
    "We hope that this tutorial has shed a useful light on how this simulation framework can be configured and used for simulating and studying complex economic problems."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "948099c6ab02a15a055545cbca87e716aee9b3e6a51d0f68b03005577a92a5b6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('ai-economist': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
